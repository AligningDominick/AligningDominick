
### ğŸŒ Aligning Dominick | Language meets safety at scale

I design stress tests for large language models â€” not in theory, but in 25+ languages that people actually speak, with risks that actually matter.

From the Swahili classroom to an Amharic health hotline, my work builds contextual evaluations that ask not just *â€œIs this aligned?â€* but *â€œIs this aligned for her, in this language, under these stakes?â€*

ğŸ“ I specialize in:
- Building multilingual safety probes rooted in real-world vignettes
- Evaluating refusal consistency, jailbreak resilience, and compliance risk
- Mapping typologically diverse language behavior to alignment metrics

My projects are open-source, reproducible, and designed for practical impact in frontier alignment.

#### âœ³ï¸ Featured Repository  
**[LLM-RiskScenes](https://github.com/AligningDominick/llm-riskscenes)**  
*Stress-testing Claude, GPT-4 and Cohere using multilingual real-world contexts across healthcare, education, and civil discourse.*

---

### ğŸ§­ Vision

Safer models wonâ€™t be built in just one language.

Alignment must scale â€” linguistically, culturally, and ethically.  
My research exists to make that scaling measurable, accountable, and community-driven.

---

ğŸ“« dominickaligning@gmail.com  
ğŸŒ Projects welcome | Multilingual collaboration encouraged
